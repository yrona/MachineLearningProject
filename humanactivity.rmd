---
title: "Machine Learning Course Project"
author: "Yilmaz Rona"
date: "January 30, 2016"
output: html_document
---

# Introduction

We analyze data collected by accelerometers on the belt, forearm, arm, and dumbell of 6 participants as they performed barbell lifts correctly and incorrectly in 5 different ways as reported in [Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). A summary of the data collection may be found on [their website](http://groupware.les.inf.puc-rio.br/har#ixzz3yn3Qh9aj).

# Analysis

## Data Download and Preprocessing

### Load Libraries
```{r loadlibs}
  library(caret)
  library(rpart)
  library(randomForest)
  library(rattle)
```

```{r displayfuncs, echo=FALSE}
  #Some functions to allow pretty display of numbers
  dispperc <- function(x) {
    return(paste(format(x*100,digits = 3),"%", sep = ""))
  }
  
  dispunit <-function(x) {
    return(format(x,format="d", big.mark=','))
  }
```
### Download

```{r cache=FALSE, echo=TRUE}
#See Appendix titled KNITR at end for the  loaded code
knitr::read_chunk('loaddata.r')
```

```{r prepdatadir, echo=FALSE}
```
```{r downloadurl, echo=FALSE}
```
```{r loaddata}
  data.subdirectory <- "data"
  skip.download <- FALSE #Set to true if you already have the data and are satisfied
  
  training.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  testing.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  data.locations <- data.frame(url = c(training.url,testing.url), 
                               file = c("training.csv","testing.csv"),
                               fullpath = c("",""),
                               row.names = c("training","testing"), 
                               stringsAsFactors=FALSE)
  
  
  for (i in 1:nrow(data.locations)) {
    data.locations[i,"fullpath"] <- downloadmyurl(data.locations[i,"url"],
                                                  data.locations[i,"file"],
                                                  data.subdirectory, 
                                                  inhibit.download = skip.download)
  }
  
  my.na.strings <-c("NA","#DIV/0!","")
    
  training.data <- read.csv(data.locations["training","fullpath"], na.strings=my.na.strings)
  testing.data  <- read.csv(data.locations["testing","fullpath"], na.strings=my.na.strings)
  
```

Training data consists of `r dispunit(nrow(training.data))` observations of `r dispunit(ncol(training.data))` variables. Testing data consists of `r dispunit(nrow(testing.data))` observations of `r dispunit(ncol(testing.data))` variables.

###Pre-Processing

```{r cache=FALSE,echo=TRUE}
#See Appendix titled KNITR at end for the  loaded code
knitr::read_chunk('prepdata.r')
```
```{r prepfunctions, echo=FALSE}
```


We first drop variables that are unhelpful: variables with too many NA values. We then remove variables that shouldn't be used as predictors because any correlation would be spurious (such as timestamps).

```{r preprocess}

#Only keep columns where more than 25% of the data is NA
good.columns <- cols.good.NA.Percentages(training.data, min.threshold = .25, cols.to.pass = c("classe"))
  
#Drop columns that can only have spurious correlations
bad.patterns <- c("timestamp","window")
bad.columns <- c("X","user_name")
for (i in bad.patterns) {
  bad.columns <- c(bad.columns,grep(i,good.columns,value = TRUE))
}
#Purge any column name appearing in vector good.columns that also is in bad.columns
good.columns <- good.columns[!good.columns %in% bad.columns]

clean.training.data <- subset(training.data,select = good.columns)
```

The columns removed to avoid training on spurious correlations were:
``` {r echo=FALSE}
  bad.columns
```

We therefore use the following `r dispunit(length(good.columns))` columns as predictors:
``` {r echo=FALSE}
  good.columns[!good.columns %in% "classe"]
```

We now partition our training data into two groups, one used to train the machine learning system, and the other to validate it (this is distinct from the testing data set we downloaded with `r dispunit(nrow(testing.data))` observations we downloaded from the Internet

```{r partition}
  set.seed(10000) # For reproducibility
  training.percentage <- 2/3 #The percentage of the data that will be used for training 
                              #The remainder will be used for model validation
  
  train.idx <- createDataPartition(clean.training.data$classe, p=training.percentage, list=FALSE)
  
  ml.training.data <- clean.training.data[train.idx, ]
  ml.testing.data <- clean.training.data[-train.idx, ]
```

To train the models used in machine learning, we use a data set with `r dispunit(nrow(ml.training.data))` observations of `r dispunit(ncol(ml.training.data))` variables. We will then validate it with `r dispunit(nrow(ml.testing.data))` observations of `r dispunit(ncol(ml.testing.data))` variables.

#Machine Learning

We will explore thre different forms of predictive modeling all based around decision trees:

1. Simplified Decision Tree Model
2. Complex Decision Tree Model
3. Random Forest Model

## A Simple Tree Model
``` {r buildtree }
  tree.model <- train(classe ~ ., method = "rpart", data = ml.training.data)
  
```

A plot of the tree appears below:
``` {r plottree }
  fancyRpartPlot(tree.model$finalModel)
```

We can immediately see a significant problem: the tree will never classify the exercise class "D" correctly.

Testing the tree using the data segment set aside for validation results in the following confusion matrix:

``` {r validatetree }
  prediction.Tree <- predict(tree.model, ml.testing.data, type = "raw")
  cfm.Tree <- confusionMatrix(prediction.Tree,ml.testing.data$classe)
  
  accuracy.Tree <- cfm.Tree$overall["Accuracy"]
  
  cfm.Tree
```

This model is not very good.  Its predictions are accurate `r dispperc(accuracy.Tree)` of the time.  It is useful, however, in being simple enough that people can make predictions by inspection rather than needing to use a computer.

## Complex Decision Tree
``` {r buildtree2 }
  tree.model2 <- rpart(classe ~ .,  data = ml.training.data, method = "class")
  
```

A plot of the tree appears below:
``` {r plottree2 }
  fancyRpartPlot(tree.model2)
```

With many more nodes, the tree ia not easily comprehended by a human being.

Testing the tree using the data segment set aside for validation results in the following confusion matrix:

``` {r validatetree2 }
  prediction.Tree2 <- predict(tree.model2, ml.testing.data, type = "class")
  cfm.Tree2 <- confusionMatrix(prediction.Tree2,ml.testing.data$classe)
  accuracy.Tree2 <- cfm.Tree2$overall["Accuracy"]
  cfm.Tree2
```

We see across the board there are improvements in specifity and sensitivity.  This more complex model, with an accuracy of `r dispperc(accuracy.Tree2)`, has better predictive power.  It is, however, less useful than the simpler model if one wants to identify the parameters that have the greatest predictive power by eye.

## Random Forest

A random forest is an ensemble of simple decision trees, built on subsets of the data. Each decision tree will provide an independent prediction given a set of predictors.  In theory, most of the decision trees will provide the correct answer, and so the most common prediction produced by the individual trees is used as the prediction of the ensemble.

``` {r buildforest }
  forest.model <- randomForest(classe ~ . , data=ml.training.data)
  prediction.forest <- predict(forest.model,ml.testing.data, type = "class")
  cfm.Forest <- confusionMatrix(prediction.forest,ml.testing.data$classe)
  
  accuracy.Forest <- cfm.Forest$overall["Accuracy"]
  cfm.Forest
```

The result is outstanding!!!  In terms of specificity and sensitivity, the random forest provides nearly perfect predictions. Its predictive accuracy is `r dispperc(accuracy.Forest)`. However, the inner workings of the model are obscured, and as a result a person cannot easily identify which predictor values constitute good performance of the exercise, and which combinations represent which common mistakes in the physical exercise.

However, for our needs, making predictions for our test set, it is precisely what we need.

#Predictions

```{r finalpredict}

  prediction.test.data <- predict(forest.model,testing.data, type = "class")
  
  prediction.test.data
```

#Appendix 1: KNITR

## Data Downloading
From loaddata.r
```{r prepdatadir, echo=TRUE, eval=FALSE}
```
```{r downloadurl, echo=TRUE, eval=FALSE}
```

## Data Preprocessing
From prepdata.r
```{r prepfunctions, echo=TRUE, eval=FALSE}
```

